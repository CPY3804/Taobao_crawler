from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import pickle
import time
import random
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def load_cookies(driver, cookies_path="taobao_cookies.pkl"):
    """è¼‰å…¥ cookies ä¸¦ç™»å…¥"""
    driver.get("https://taobao.com")  # âš ï¸ å¿…é ˆæ‰“é–‹å°çš„ domain
    time.sleep(2)

    with open(cookies_path, "rb") as f:
        cookies = pickle.load(f)

    for cookie in cookies:
        # é¿å… selenium æ‹‹éŒ¯ï¼šåˆªæ‰ cookie ä¸­çš„ 'sameSite' èˆ‡ 'expiry'
        cookie.pop('sameSite', None)
        cookie.pop('expiry', None)
        try:
            driver.add_cookie(cookie)
        except Exception as e:
            print(f"âš ï¸ Cookie åŠ è¼‰å¤±æ•—: {cookie.get('name')}ï¼ŒåŸå› ï¼š{e}")

    driver.refresh()
    time.sleep(3)
    print("âœ… Cookies å·²åŠ è¼‰ï¼Œé é¢å·²åˆ·æ–°")

def search_product(driver, keyword):
    """æ ¹æ“šé—œéµå­—æœå°‹å•†å“"""
    search_input = driver.find_element(By.ID, "q")
    search_input.clear()
    search_input.send_keys(keyword)
    time.sleep(random.randint(1, 3))
    driver.find_element(By.XPATH, '//*[@id="J_TSearchForm"]/div[2]/button').click()
    print(f"âœ… å·²æœå°‹ã€Œ{keyword}ã€")


def scrape_products(driver, max_count=10):
    """æŠ“å–æœå°‹çµæœé é¢ä¸Šçš„å•†å“è³‡è¨Š"""
    # ç­‰å¾…å•†å“å…ƒç´ è¼‰å…¥ï¼ˆclass="item" æ˜¯æ¯å€‹å•†å“å¤–å±¤ï¼‰
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "item")]'))
        )
    except:
        print("âŒ ç­‰å¾…å•†å“è¼‰å…¥è¶…æ™‚")
        return []

    # åˆ‡æ›åˆ°æ–°åˆ†é ï¼ˆæœ€å¾Œä¸€å€‹ï¼‰
    driver.switch_to.window(driver.window_handles[-1])

    products = driver.find_elements(By.XPATH, '//div[@id="content_items_wrapper"]/div') #æ‰€æœ‰divæ¨™ç±¤

    results = []
    for idx, product in enumerate(products[:max_count]): #äºŒæ¬¡æå–
        try:
            title_el = product.find_element(By.XPATH, './/div[contains(@class, "title")]')
            price_el = product.find_element(By.XPATH, './/div[contains(@class, "priceInt")]')
            link_el = product.find_element(By.XPATH, './/a')  # å‡è¨­ a æ¨™ç±¤æ˜¯åŒ…å«é€£çµçš„åœ°æ–¹

            title = title_el.text.strip()
            price = price_el.text.strip()
            link = link_el.get_attribute('href')
            results.append({
                "title": title,
                "price": price,
                "link": link
            })


        except Exception as e:
            print(f"âŒ ç¬¬ {idx+1} ç­†è³‡æ–™å‡ºéŒ¯ï¼š{e}")
            continue


    print("\nğŸ“¦ æŠ“åˆ°å•†å“è³‡è¨Šå¦‚ä¸‹ï¼š\n")
    for i, item in enumerate(results, 1):
        print(f"{i}. [{item['title']}]")
        print(f"   ğŸ’° åƒ¹æ ¼ï¼š{item['price']} å…ƒ")
        print(f"   ğŸ”— é€£çµï¼š{item['link']}\n")
    return results



if __name__ == "__main__":
    driver = webdriver.Chrome()
    driver.maximize_window()
    driver.implicitly_wait(10)

    load_cookies(driver)

    word = input("è«‹è¼¸å…¥è¦æœå°‹çš„å•†å“ï¼š")

    search_product(driver, word)

    scrape_products(driver, max_count=10)


    input("è«‹æŒ‰ Enter é—œé–‰ç€è¦½å™¨...")
    driver.quit()
