from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import pickle
import time
import random
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd  # æ–°å¢ pandas å¥—ä»¶çš„å°å…¥


#pip install selenium pandas openpyxl

def load_cookies(driver, cookies_path="taobao_cookies.pkl"):
    """è¼‰å…¥ cookies ä¸¦ç™»å…¥"""
    driver.get("https://taobao.com")  # âš ï¸ å¿…é ˆæ‰“é–‹å°çš„ domain
    time.sleep(2)

    with open(cookies_path, "rb") as f:
        cookies = pickle.load(f)

    for cookie in cookies:
        # é¿å… selenium æ‹‹éŒ¯ï¼šåˆªæ‰ cookie ä¸­çš„ 'sameSite' èˆ‡ 'expiry'
        cookie.pop('sameSite', None)
        cookie.pop('expiry', None)
        try:
            driver.add_cookie(cookie)
        except Exception as e:
            print(f"âš ï¸ Cookie åŠ è¼‰å¤±æ•—: {cookie.get('name')}ï¼ŒåŸå› ï¼š{e}")

    driver.refresh()
    time.sleep(3)
    print("âœ… Cookies å·²åŠ è¼‰ï¼Œé é¢å·²åˆ·æ–°")

def search_product(driver, keyword):
    """æ ¹æ“šé—œéµå­—æœå°‹å•†å“"""
    search_input = driver.find_element(By.ID, "q")
    search_input.clear()
    search_input.send_keys(keyword)
    time.sleep(random.randint(1, 3))
    driver.find_element(By.XPATH, '//*[@id="J_TSearchForm"]/div[2]/button').click()
    print(f"âœ… å·²æœå°‹ã€Œ{keyword}ã€")


def scrape_products(driver):
    """æŠ“å–æœå°‹çµæœé é¢ä¸Šçš„æ‰€æœ‰å•†å“è³‡è¨Š"""
    try:
        # ç­‰å¾…å•†å“å…ƒç´ è¼‰å…¥ï¼ˆclass="item" æ˜¯æ¯å€‹å•†å“å¤–å±¤ï¼‰
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "item")]'))
        )
    except Exception as e:
        print(f"âŒ ç­‰å¾…å•†å“è¼‰å…¥è¶…æ™‚ï¼š{e}")
        return []

    # åˆ‡æ›åˆ°æ–°åˆ†é ï¼ˆå¦‚æœæœå°‹çµæœæ˜¯åœ¨æ–°åˆ†é ï¼‰
    driver.switch_to.window(driver.window_handles[-1])

    # ç²å–æ‰€æœ‰å•†å“å…ƒç´ 
    products = driver.find_elements(By.XPATH, '//div[@id="content_items_wrapper"]/div')  # æ‰€æœ‰å•†å“çš„ div æ¨™ç±¤

    results = []
    for idx, product in enumerate(products):  # ç§»é™¤ max_count é™åˆ¶
        try:
            title_el = product.find_element(By.XPATH, './/div[contains(@class, "title")]')
            price_el = product.find_element(By.XPATH, './/div[contains(@class, "priceInt")]')
            link_el = product.find_element(By.XPATH, './/a')  # å‡è¨­ a æ¨™ç±¤åŒ…å«å•†å“é€£çµ

            title = title_el.text.strip()
            price = price_el.text.strip()
            link = link_el.get_attribute('href')
            results.append({
                "title": title,
                "price": price,
                "link": link
            })

        except Exception as e:
            print(f"âŒ ç¬¬ {idx+1} ç­†è³‡æ–™å‡ºéŒ¯ï¼š{e}")
            continue

    print("\nğŸ“¦ æŠ“å–åˆ°çš„å•†å“è³‡è¨Šå¦‚ä¸‹ï¼š\n")
    for i, item in enumerate(results, 1):
        print(f"{i}. [{item['title']}]")
        print(f"   ğŸ’° åƒ¹æ ¼ï¼š{item['price']} å…ƒ")
        print(f"   ğŸ”— é€£çµï¼š{item['link']}\n")

    # å°‡çµæœä¿å­˜åˆ° Excel æ–‡ä»¶
    if results:
        df = pd.DataFrame(results)  # å°‡çµæœè½‰æ›ç‚º pandas çš„ DataFrame
        df.to_excel("å•†å“è³‡æ–™"+word+".xlsx", index=False)  # ä¿å­˜ç‚º Excel æ–‡ä»¶
        print("âœ… å•†å“è³‡è¨Šå·²æˆåŠŸå°å‡ºè‡³å•†å“è³‡æ–™.xlsx")
    else:
        print("âš ï¸ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•å•†å“è³‡è¨Šï¼Œç„¡æ³•ç”Ÿæˆ Excel æ–‡ä»¶ã€‚")

    return results




if __name__ == "__main__":
    driver = webdriver.Chrome()
    driver.maximize_window()
    driver.implicitly_wait(10)

    load_cookies(driver)

    word = input("è«‹è¼¸å…¥è¦æœå°‹çš„å•†å“ï¼š")

    search_product(driver, word)

    scrape_products(driver)


    input("è«‹æŒ‰ Enter é—œé–‰ç€è¦½å™¨...")
    driver.quit()
